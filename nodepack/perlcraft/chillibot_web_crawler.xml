<node>
  <doctext>#!/usr/local/bin/perl

# This program uses the LWP module to read a web page
# and store its URL, Title, Description and Keywords in
# a database. It then follows the links on the page and
# processes any other pages it finds. The following 
# switches are used to control how the program operates:
# 	/u specify the url to start at
#	/c continue from the last saved restart point
#	/f follow links on page and process pages found
#	/r write next page to process to a restart file
#	/d update the database
#	/l produce logfile of crawl
#   either /u or /c must be specified.
#
# This is my first attempt at a reasonable program in Perl
# so please feel free to criticise to your hearts content
# (its the only way I will learn).

use HTML::TreeBuilder;
use LWP::RobotUA;
use URI::URL;
use DBI;

# Setup user agent for robot

$ua = LWP::RobotUA-&gt;new('ChilliBot/0.1.' . LWP::Version,'ChilliBot@nextmilltech.co.uk');

$ua-&gt;delay(1);

print $ua-&gt;agent;
print &quot;\n&quot;;

# check if any arguements entered

if(scalar(@ARGV) &gt; 0)
{
	my $startpage=&quot;&quot;;

	local @visitedpages;
	local $logtofile = 0;
	local $drilldown = 0;
	local $usedatabase = 0;
	local $setforrestart =0;
	local $continue=0;
	local $logfilename;

	$|=1; # set autoflush on for logfile

	# check parameters and set appropriate variables.

	for(my $i=0;$i&lt;scalar(@ARGV);$i++)
	{
		if($ARGV[$i] eq &quot;/f&quot;)
		{
			$drilldown = 1;
		}
		if($ARGV[$i] eq &quot;/l&quot;)
		{
			$logtofile = 1;
			$logfilename=$ARGV[$i+1];
		}
		if($ARGV[$i] eq &quot;/u&quot;)
		{
			$startpage=$ARGV[$i+1];
		}
		if($ARGV[$i] eq &quot;/d&quot;)
		{
			$usedatabase = 1;
		}
		if($ARGV[$i] eq &quot;/r&quot;)
		{
			$setforrestart = 1;
		}
		if($ARGV[$i] eq &quot;/c&quot;)
		{
			$continue= 1;
		}

	}

	if($logtofile==1)
	{
		open(LOGFILE,&quot;&gt;$logfilename&quot;) || die &quot;Cannot open logfile $logfilename\n&quot;;
		close(LOGFILE);
	}


	# we do not want to visit pages already visited so keep an array
	# of their URLs

	@visitedpages=();

	if($usedatabase==1)
	{
		# if we are using the database then add all URLs from it
		# to the list of visited pages.

		print &quot;Building visited pages list...&quot;;

		my $DSN = &quot;DSN=PageData&quot;;

		my $dbh  = DBI-&gt;connect(&quot;dbi:ODBC:$DSN&quot;) || die &quot;$DBI::errstr\n&quot;;

		my $sql_handle=$dbh-&gt;prepare(&quot;SELECT PageURL FROM Page&quot;) || die $dbh-&gt;errstr;

		$sql_handle-&gt;execute() || die $dbh-&gt;errstr;

		while ( @row = $sql_handle-&gt;fetchrow_array() )
		{
			push(@visitedpages,$row[0]);	
		}

		$dbh-&gt;disconnect();
		
		print &quot;done\n&quot;;
	}

	if($continue==1)
	{
		# if we are continuing then find which page to continue from

		open(CONTINUE,&quot;restartwith.txt&quot;) || die &quot;Cannot open restart file\n&quot;;
		my @continueeurl=&lt;CONTINUE&gt;;
		foreach (@continueeurl)
		{
			$startpage=$_;
		}
		close(CONTINUE);
	}

	if($startpage ne &quot;&quot;)
	{
		&amp;gethtml($startpage);
	}




}
else
{
	# No parameters entered so printout the usage information

	print &quot;Usage:\n&quot;;
	print &quot;    perl robot.pl [/u start_url] [/f] [/d] [/r] [/c] [/s] [/l logfile]\n&quot;;
	print &quot;        where /u - url to start crawl from\n&quot;;
	print &quot;              /f - follow links on each page\n&quot;;
	print &quot;              /d - add page details to database\n&quot;;
	print &quot;              /r - save last accessed url for restart with /c\n&quot;;
	print &quot;              /c - continue from last restart-saved url\n&quot;;
	print &quot;              /l - output to logfile\n\n&quot;;
	print &quot;	       note: either /u or /c must be specified\n\n&quot;;
}
print (&quot;Run Complete\n&quot;);


# main routine

sub gethtml
{

	my $html;
	my $treeparse;

	my $rlink;
	my @linkarray;
	my $baseurl;
	my $pagealreadyvisited;
	my $pagetoprocess;
	my $rlinkarray;

	local $pagetitle =&quot;&quot;;
	local $pagedescription = &quot;&quot;;
	local $pagekeywords=&quot;&quot;;
	local $pagebaseurl=&quot;&quot;;

	$pagetoprocess = $_[0];

	if($setforrestart==1)
	{
		# write URL to restart file.

		open(RESTARTFILE,&quot;&gt;restartwith.txt&quot;) || die &quot;Cannot open restart file\n&quot;;
		print RESTARTFILE $pagetoprocess;
		close(RESTARTFILE);
	}

	# check we have not already visited this page

	$pagealreadyvisited=0;

	foreach (@visitedpages)
	{
		if($_ eq $pagetoprocess)
		{
			$pagealreadyvisited=1;
		}

	}

	if ($pagealreadyvisited == 0)
	{
		print &quot;Processing: $_[0]...&quot;;

		# request the page

		$request = HTTP::Request-&gt;new('GET', $_[0]);		
		$response = $ua-&gt;request($request); 	

		if ($response-&gt;is_success) 
		{		    
			if($logtofile==1)
     			{
				open(LOGFILE,&quot;&gt;&gt;$logfilename&quot;) || die &quot;Cannot open logfile $logfilename\n&quot;;
     				print LOGFILE &quot;Processing: $_[0]...Response OK\n&quot;;
				close(LOGFILE);
		  	}
			
			# parse retrieved HTML
			
			@linkarray=();
			$html=$response-&gt;content;
			$treeparse=HTML::TreeBuilder-&gt;new;
			$treeparse-&gt;parse($html);
			
			# extract anchor links

			$rlinkarray=$treeparse-&gt;extract_links(&quot;a&quot;);

			# call treewalker function to check meta tags

			$treeparse-&gt;traverse(\&amp;treewalker);
			$treeparse-&gt;delete();
			$pagebaseurl=$response-&gt;base;

			if($logtofile==1)
		     			{
						open(LOGFILE,&quot;&gt;&gt;$logfilename&quot;) || die &quot;Cannot open logfile $logfilename\n&quot;;
		     				print LOGFILE &quot;    Title: $pagetitle\n&quot;;
		     				print LOGFILE &quot;    Description: $pagedescription\n&quot;;
		     				print LOGFILE &quot;    Keywords: $pagekeywords\n&quot;;
						print LOGFILE &quot;    Base URL: $pagebaseurl\n&quot;;
						close(LOGFILE);
		     			}
	
			if($usedatabase==1)
			{

				# write page details to database

				# DBI::ODBC falls over with any string
				# longer than 255

				if(length($pagetitle)&gt;255)
				{
					$pagetitle=substr($pagetitle,0,255);
				}

				if(length($pagedescription)&gt;255)
				{
					$pagedescription=substr($pagedescription,0,255);
				}
				
				if(length($pagekeywords)&gt;255)
				{
					$pagekeywords=substr($pagekeywords,0,255);
				}
								
				my $DSN = &quot;DSN=PageData&quot;;

				my $dbh  = DBI-&gt;connect(&quot;dbi:ODBC:$DSN&quot;) || die &quot;$DBI::errstr\n&quot;;

				my $sql_handle=$dbh-&gt;prepare(q{
						INSERT INTO Page (PageURL, Title, Description,Keywords) VALUES (?, ?, ?, ?)
						}) || die $dbh-&gt;errstr;

				$sql_handle-&gt;execute(&quot;$_[0]&quot;,&quot;$pagetitle&quot;,&quot;$pagedescription&quot;,&quot;$pagekeywords&quot;)
						    || die $dbh-&gt;errstr;

				$dbh-&gt;disconnect();

			}

			# add page to visited pages array

			push(@visitedpages,$_[0]);	

			print &quot;OK\n&quot;;

			# convert links from a referenced array to
			# a normal array
			
			foreach $rlink(@$rlinkarray)
			{
	
				push(@linkarray,$$rlink[0]);	
			}

			# create full URLs from links

			$baseurl=$response-&gt;base;
			@linkarray = map { $_= url($_, $baseurl)-&gt;abs; } @linkarray;

			foreach (@linkarray)
			{
				if($logtofile==1)
		     		{
					open(LOGFILE,&quot;&gt;&gt;$logfilename&quot;) || die &quot;Cannot open logfile $logfilename\n&quot;;
		     			print LOGFILE &quot;    $_\n&quot;;
					close(LOGFILE);
		     		}

			}
		
			# put in seperate loop so that printout is correct
			foreach (@linkarray)
			{
				# if link is http and does not contain
				# any odd charcters then call this function
				# recursively passing in the link

				if (/http:/i)
				{
					if (/[#\@\$]/)
					{
					}
					else
					{
						if($drilldown == 1)
						{
							&amp;gethtml($_);
						}
					}
				}
			}
		} 
		else 
		{
		     print &quot;Failed\n&quot;;
		     if($logtofile==1)
     		     {
			open(LOGFILE,&quot;&gt;&gt;$logfilename&quot;) || die &quot;Cannot open logfile $logfilename\n&quot;;
     			print LOGFILE &quot;Processing: $_[0]...Failed\n&quot;;
			close(LOGFILE);
		     }
		}
	}

}

# Used to find title tag, and description and keyword metatags.
sub treewalker
{
	my ($node, $start, $depth) = @_;
        if (ref $node) 
	{
             my $tag = $node-&gt;tag;

	     if ($tag eq &quot;meta&quot;)
	     {
		my $metaname=$node-&gt;attr(&quot;name&quot;);
		if ($metaname ne &quot;&quot;)
		{
			if ($metaname=~ /description/i)
			{
				my $description=$node-&gt;attr(&quot;content&quot;);

				# remove CR and LF from description.
				$description =~ s/\n/ /sg;
				$description =~ s/\r/ /sg;
				$pagedescription = $description;		
			}
			if ($metaname=~ /keywords/i)
			{
				my $keywords=$node-&gt;attr(&quot;content&quot;);

				# remove CR and LF from description.
				$keywords =~ s/\n/ /sg;
				$keywords =~ s/\r/ /sg;
				$pagekeywords = $keywords;		
			}
		}
     	     }     
	
	     if ($tag eq &quot;title&quot; &amp;&amp; $pagetitle eq &quot;&quot;)
	     {
		my $contentnodearray=$node-&gt;content;
		foreach my $contentnode(@$contentnodearray)
		{
			if (not ref $contentnode)
			{
				$pagetitle=$contentnode;		
			}
		}
	     }
	
	

	}
	return 1; # This makes it recurse through all sub nodes
}
 </doctext>
  <type_nodetype>121</type_nodetype>
  <node_id>49276</node_id>
  <author_user>37684</author_user>
  <title>ChilliBot Web Crawler</title>
  <createtime>2001-01-02 09:56:24</createtime>
  <nodeupdated>2005-07-21 13:46:57</nodeupdated>
</node>
